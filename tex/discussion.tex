\section{Discussion}
\label{discussion}
In this section, we discuss the learned lessons during the implementation of our approaches, the generalizability of our study, the application of our approach for configuration interactions.

\subsection{Generalizability of our study}

Our approach is not designed for any specific programming language or any type of system. Adding more case studies can benefit generalizability, but it may not address the generalizability issue. Below, we discuss some aspects that may impact the generalizability of our study.

\noindent \textbf{Option quantity.} Our approach aims to predict the inconsistent option performance variation issue automatically. We find that there are 365 and 162 configuration options in \emph{Hadoop} and \emph{Cassandra}, respectively. If the number of configuration option in a system is small, e.g., less than ten options, our approach may not have a practical impact for practitioners, as practitioners can examine the small limited number of configuration options by manually.

\noindent \textbf{Test and Option coverage.} Test and Option coverage. Our approach depends on the readily available small-scale tests in the software systems. If the tests cannot cover the source changes and impacted options, our approach may fail to predict \inconsistent.  However, since our approach works at the commit level, only the changed methods need to be covered by the test. We find that for all the changed methods in all commits, 67.97\% and 53.3\% are covered by the tests in \emph{Hadoop} and \emph{Cassandra}, respectively. Such changed methods cover a total of 122 and 55 options in \emph{Hadoop} and \emph{Cassandra}, respectively. Such high coverage ensures the success of our approach. This also implies that, in order to adopt our approach, practitioners may first evaluate whether the source code that is likely to be changed is covered by tests.

\noindent \textbf{Test quality.} We use the existing available small-scale tests to evaluate the performance variations. Prior  research~\cite{DBLP:conf/wosp/HorkyLMST15,ding2020towards} study the use of performance unit tests to increase performance awareness. If the existing test is written with a sub-optimal quality, the performance results may be biased. For example, the test failures in the flaky test may introduce noise and require extra running time to achieve the needed repetition. Recent research~\cite{ding2020towards} discusses the reasons for tests not suitable for performance evaluation, which can be leveraged to know how well another project can adopt our approach. 



\subsection{The application of our approach for predicting \inconsistent in interaction of configuration options}
Based on the findings from study~\cite{RN2864}, performance bugs are often due to configurations. In particular, the results from study~\cite{RN2864} show that the majority (72\%) of parameter configuration bugs is related to only one option;about 28\% of studied configuration bugs involve two or more configuration options. 
Therefore, on the one hand, our approach can be directly used to predict the majority of configuration-aware performance issues. On the other hand, our measured data and our approach can be also partially toned to predict a combination of configuration-aware performance issues. We have executed 61,860 \instance instances. Even if our measured data cannot represent all the interactions, our measured performance data covers part of interactions of options, like two-way, and N-way options. For example, we assume that there are two options O1 and O2. The possible values of O1 is 0, 4, 8, defaulting to 0. And the possible values of O2 is True and False, defaulting to False. Our performance data covers the following pairwise option values between O1 and O2: 1) $<0, False>$ 2) $<4, False>$ 3) $<8, False>$ 4) $<0, True>$. On the other hand, our existing performance data misses the following pairwise option values: 1) $<4, True>$ 2) $<8, True>$. N-way testing is a kind of combinational test which requires that every combination of any N parameter values in the software must be tested at least once.

\begin{comment}
Based on the finding from study~\cite{RN2864}, we know performance bugs are often due to configurations. In particular, the results from study [1] show that the majority (72\%) of parameter configuration bugs is related to only one option. And about 28\% of studied configuration bugs involve two and more configuration options. 
Therefore, on one hand, our approach can be directly used to predict the majority of configuration-aware performance issues. On the other hand, our measured data and our approach can be also partially toned to predict a combination of configuration-aware performance issues. We have executed 61,860 \instance instances. Each execution represents a combination of options. Even if our measured data cannot represent all the interactions, our measured performance data covers part of interactions of options, like two-way, and N-way options. 

In addition, our approach can be directly used to predict \inconsistent in interaction of options. 
Similar to our existing approach, we can first calculate or sample~\cite{DBLP:conf/icse/MedeirosKRGA16} the combinations of configuration options (we take a combination of two configuration options as an example). Second, we run and measure the performance data for each two-way interaction of options, based on the sampling. Third, we label each combination of configuration options using the steps mentioned in subsection 3.2.6 and 3.2.7. Next, we can directly use our approach to extract four dimensions of metrics. The only difference is that such metrics are related to a combination of options instead of only one impacted option. Finally, we can still use machine learning-based algorithms to build the model to predict \inconsistent issues, for a combination of configuration options.
\end{comment}

\subsection{The difference between prior research and our study}
% The aim of our prediction in RQ1 is to reduce the effort of conducting configuration-aware performance testing. Several prior approaches exist for such task~\cite{DBLP:conf/sigsoft/KimMKBSBd13,DBLP:conf/icse/MedeirosKRGA16}. SPLat~\cite{DBLP:conf/sigsoft/KimMKBSBd13} proposes a novel lightweight analysis to reduce the number of configurations for each test, to further reduce the testing time. The authors abstract feature model to find the configuration constraint, and perform lightweight dynamic analysis to find shared execution paths to reduce combinations of configurations in the testing configurable system. The difference between SPLat and our study is that SPLat aims to reduce test execution time. SPLat uses feature models and dynamic analysis to determine which test must be run, to effectively cover a set of configurations. Another difference is that SPLat only studies boolean configuration options. 
% Medeiros et al.~\cite{DBLP:conf/icse/MedeirosKRGA16} conduct a comparative study of 10 sampling approaches, and their combinations regarding their capability to detect configuration-related faults and the size of sample sets under four assumptions. The authors find that among the sample algorithms, the larger sample size is, the more faults can be detected. For small sample sets, the algorithm most-enabled-disabled performs better. Different from our study, Medeiros et al. aim to study the best sampling algorithm to reduce the sample set, and can still detect faults. 
The aim of our prediction in RQ1 is to reduce the effort of conducting configuration-aware performance testing. Several prior approaches exist for such a goal~\cite{DBLP:conf/sigsoft/KimMKBSBd13,DBLP:conf/icse/MedeirosKRGA16}. SPLat~\cite{DBLP:conf/sigsoft/KimMKBSBd13} proposes a novel lightweight analysis to reduce the number of configurations for each test, to further reduce the testing time. The authors abstract feature models to find the configuration constraint, and perform lightweight dynamic analysis to find shared execution paths to reduce combinations of configurations in the testing configurable system. The difference between SPLat and our study is that SPLat aims to reduce test execution time. SPLat uses feature models and dynamic analysis to determine which test must be run, to effectively cover a set of configurations. Another difference is that SPLat only studies boolean configuration options. 
Medeiros et al.~\cite{DBLP:conf/icse/MedeirosKRGA16} conduct a comparative study of 10 sampling approaches and their combinations regarding their capability to detect configuration-related faults and the size of sample sets. The authors find that among the sample algorithms, the larger the sample size is, the more faults can be detected. For small sample sets, the algorithm most-enabled-disabled performs better. Different from our study, Medeiros et al. aim to study the best sampling algorithm to reduce the sample set that can still detect faults.  Five of the sampling algorithms, one-enable, one-disable, most-enable-disable, statement-coverage, and random sample cannot cover all the combinations of options. Five of the sampling algorithms are t-wise, and t ranges from 2 to 6, i.e., 2-wise, 3-wise, 4-wise, 5-wise, and 5-wise. Such t-wise sampling algorithms use covering array table to select configurations. However, for performance testing, the covering array approach likely does not work since we want to isolate each combination's performance impact from others.

