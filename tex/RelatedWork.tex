\section{Related Work}
\label{sec:relatedwork}

In this section, we discuss prior work along three dimensions: software performance regression detection, performance model for configurable system, and identifying optimal configuration for performance. % engineering.

\subsection{Performance regression detection}
Extensive prior research has proposed automated techniques to detect performance regressions. Such detection techniques can be divided into two categories: measurement-based and model-based detection. 

Measurement-based approaches compare performance metrics (e.g., CPU usage) between two consecutive versions to detect performance regressions. %measure performance metrics and compares these performance metrics between two consecutive versions of a system to detect performance regression. 
For example, Nguyen \emph{et al$.$}~\cite{Nguyen:2012:ADP,nguyen2011automated,Nguyen:2014:ICS} %conducted a series of studies on performance regressions. Nguyen \emph{et al$.$} apply control charts to analyze performance counters across test runs to detect performance regression automatically. They construct the control chart to detect performance regressions  by setting upper and lower bounds of performance counters.
leveraged control charts to identify performance regressions. %~\heng{treat regression as a countable word throughout the paper, countable seems better}. 
A control chart has an upper control limit and a lower control limit. A performance regression is detected when a performance metric is above the upper limit or below the lower limit. Foo \emph{et al$.$}~\cite{foo2010mining} proposed an approach that compares a test's performance metrics to historical performance metrics. %\med{from what?}performance regression testing repositories to detect potential performance regressions. 

A model-based approach builds %\med{This paragraph is difficult to understand. What is detected model? is it prediction model? and what is counters? what is signatures? what do you mean by heterogeneous environment?} 
%\jinfu{detected model is a general model, we can use model. Performance counter is performance metric, like CPU usage. Some papers use signatures to represent system users' behavior.} \med{I updated this paragraph, though Bodik work is not clear} 
a machine learning model with a set of performance metrics to detect performance regressions.  Cohen et al.~\cite{Cohen:2005:CIC} showed an implication that it is ineffective and not enough to index and identify performance problems with simple records of raw system metrics. Cohen et al. used TAN (Tree-Augmented Bayesian Network) models to model the system performance states based on a small subset of metrics. %Therefore, the authors present an approach to capture signatures representing system states from a running system and cluster such signitures to detect recurrent or similar performance problems. 
Bodik \emph{et al$.$}~\cite{bodik2008hilighter} leveraged a logistic regression model to model system users' behavior to improve Cohen \emph{et al$.$}\textquotesingle s model. %~\heng{need to mention Cohen's model beforehand}. 
Foo \emph{et al$.$}~\cite{DBLP:conf/icse/FooJAHZF15} proposed an approach that uses ensembles of models to detect performance regressions in heterogeneous environments (e.g., different hardware and software configurations). % \med{examples of what heterogeneous environments} \jinfu{different hardware and software configurations}). 
Xiong \emph{et al$.$}~\cite{Xiong:2013:VAM} proposed a model-driven framework to diagnose application performance and identify the root cause of performance issues. %Such framework uses linear regression to build the predict model to automatically diagnose the system performance in cloud environment and lead to root cause of performance problem.

%\med{well, we predict it, so it's kind of detecting performance regression}Our research is not designed to detect performance regression. The goal of our research is to examine the impact of configuration options on the performance regression. In our paper, we conduct measurement-based approach to identify performance regression on the commit level.

%\med{is the following paragraph correct?}
Our work complements this line of research in the sense that we consider the configuration aspect of highly configurable software systems. For instance, a code change might not show a performance regression on the default configuration, while leading to regressions on other configurations. This paper sheds light on the \inconsistent problem by first quantifying the existence of inconsistent performance variations, then proposing a prediction model that identifies the commits, tests, and options that exhibit the \inconsistent problem. 


\subsection{Performance model for configurable system}
Many prior research has been conducted on predicting performance for configurable software systems. 
M{\"{u}}hlbauer et al.~\cite{DBLP:conf/kbse/MuhlbauerAS19,DBLP:conf/kbse/MuhlbauerAS20} build performance model to identify performance changes in software performance evolution. The authors combine active learning and Gaussian process regression to model and estimate a software system’s performance evolution with only a few revision measurements. Such prior study provides an evidence that performance changes during software evolution, which motivates our study. Jamshidi et al.~\cite{DBLP:conf/kbse/JamshidiSVKPA17,DBLP:conf/icse/JamshidiVKSK17,DBLP:conf/sigsoft/JamshidiVKS18} employ transfer learning to learn performance model across environments. Work~\cite{DBLP:conf/kbse/JamshidiSVKPA17} conducts an exploratory study on four systems to investigate how a performance model for a configurable system changes when the system is deployed in different environments. The result shows that for severe environment changes, a considerable part of invalid configuration is preserved across environmental changes. Following the exploratory study, Jamshidi et al.~\cite{DBLP:conf/icse/JamshidiVKSK17} propose an approach to learn the performance model for a real system using other sources such as simulator with a few measurements at a lower cost. The results show that transfer learning can achieve high prediction accuracy with only a few samples from the real-world system. Jamshidi et al.~\cite{DBLP:conf/sigsoft/JamshidiVKS18} propose learning to sample (L2S) that extract transferable knowledge from the source environment to help sample the configurations in the target environment. DeepPerf~\cite{DBLP:conf/icse/HaZ19} uses a deep feed-forward neural network to model configurable software system. Results show that DeepPerf can predict a system's performance with a small sample of configuration. 

The existing studies use the historical revisions’ performance, or the sampled configuration’s performance to build a model, to estimate the performance of future revision. Different from prior study, we measure all the configuration options and collect real performance data. In addition, we aim to find the inconsistent option performance variations during software evolution. In general, our study is orthogonal to the above approaches and our measured performance data can be used in future research on performance.

\subsection{Identifying optimal configuration for performance} 
A large body of research has been conducted on performance optimization by finding optimal configuration. 
Siegmund et al.~\cite{RN2880} build mathematical models that describe the impact of a configuration on software performance based on each option's value. Raghavachari et al.~\cite{RN3537} propose an iterative approach to identify an optimal configuration in terms of performance. Their approach consists of selecting for a J2EE web application a first configuration, compare its performance to a second configuration until the optimal configuration is found. Similarly, Dia et al.~\cite{RN3543} propose an approach that automatically adjusts the values of existing configuration options at run-time to optimize the CPU and memory usage objectives.
Li et al.~\cite{LiAutoConfig} leveraged performance monitoring data and execution logs to dynamically optimize the values of performance-related configuration options according to varying workloads in the field. 
Guo et al.~\cite{RN3544} leverage non-linear regression to suggest an optimal configuration.
It was extended to DECART~\cite{DBLP:journals/ese/GuoYSASVCWY18} that combines the CART with automated resampling and parameter tuning to predict performance of configurable systems. However, collecting a large amount of data for training a model that predicts the performance of a configuration is expensive.
Therefore, Sarkar et al.~\cite{RN3089} evaluated the progressive and projective sampling to train a model that predicts the performance of configuration. For their initial training sample, they consider data on which each option is enabled at least once. 
Nair et al.~\cite{DBLP:conf/sigsoft/NairMSA17,DBLP:journals/tse/Nair0MSA20} conduct several studies to find good performing configurations. 
Nair et al.~\cite{DBLP:conf/sigsoft/NairMSA17} first propose a rank-based approach that uses the performance model building from a few sample configurations, to rank the configurations, in order to find the faster configurations. 
Nair et al.~\cite{DBLP:journals/tse/Nair0MSA20} also propose a novel approach named FLASH that use a sequential model-based approach to find better configurations for a software system. 
Oh et al.~\cite{DBLP:conf/sigsoft/OhBMS17} propose a true random sampling to search configurations recursively to find near-optimal configurations without building a performance model.
Other efforts identified the optimal configuration options in terms of performance by leveraging existing optimization approaches, i.e., iterative search~\cite{RN3545}, multi-objective optimization~\cite{singh2016optimizing}, and smart hill climbing~\cite{RN3518}.

The goal of our paper is not to identify optimal configuration options or to debug an existing performance-related configuration error. Instead, we focus on studying inconsistent option performance through different commits. In particular, we focus on understanding whether a performance improvement or regression is consistent through all the values of an option. That is important, as one can improve the performance of a software system or release new changes that do not impact the performance under one configuration when other configurations hide a performance regression. 

Furthermore, prior work on this line of research compares the absolute performance between two values for the same option, while this can be subjective, as discussed earlier. One option's value can naturally consume performance as it enables the execution of additional features. However, performance comparison need also consider historical performance data. For instance, the execution of the software system under the same option's value can be improved %\bram{for those features? flow in this paragraph is unclear} 
compared the same option and value prior to that commit. In addition, a better performing option's value can show a regression compared to the prior commit as well. %impact of configuration options on the performance regression.% While a new commit does not show any performance regression under the default configuration, other configurations might show a low performance when the same configuration showed a good performance in the prior commit. 

%\subsection{Software Configuration}

%A large body of research efforts have been conducted on software configuration, which mainly focus on understanding configuration problems, preventing configuration errors, and debugging configuration errors. In this Section, we focus on the efforts that are related to the performance of configuration, while we refer to our prior systematic literature review~\cite{tse} for more details. %Few research efforts consider the performance aspect of software configuration \bram{wasn't there a lot of work on performance tuning in Mohammed's TSE survey?}. 

%\subsubsection{Understanding Configuration Problems}
%Configuration makes a software system complex~\cite{tse}, which leads to configuration errors that are severe, common, and hard to debug~\cite{RN3251}. For instance, Jin et al.~\cite{RN2897} found that configuration options add more complexity to the development and testing of highly configurable software systems. Han et al.~\cite{RN2864} found that configuration options are responsible for 59\% of the performance bugs. Gousios et al.~\cite{RN3551} observed that the configuration of the garbage collectors have an impact on the performance of server applications. Furthermore, Sayagh et al.~\cite{RN3249, RN2758} found that the impact of a configuration option can spread to multiple layers of an architectural stack.% Jin et al.~\cite{RN2897} found that there is a need for tools that debug configuration errors in multi-languages software systems. 

%Our work is different from this line of research as we consider the performance regression that is caused by configuration options.
%\subsubsection{Debugging Configuration Errors} 
%A second line of research proposed and evaluated different approaches to identify misconfigured configuration options. Dong et al.\cite{RN2805, RN3163} leverage the slicing technique to identify the misconfigured option for a given error message or exception. Rabkin et al.~\cite{RN2822} leverage a data flow analysis technique to identify for each option, which source code lines it might impacts. Attaryian et al.~\cite{RN3248} combined dynamic control and data flow analysis to identify misconfigured options. Zhang et al.~\cite{RN2839, RN2777} compared the trace of a correct execution against the trace of an incorrect execution to identify culprit options. We refer to our prior systematic literature review~\cite{tse} and the work of Tianyin et al.~\cite{RN3252} and Andrzejak et al.\cite{andrzejaksoftware} for further details about the existing configuration debugging approaches. 

%Our work is different from this line of research since we do not consider debugging configuration errors, but understanding and identifying performance regressions that are caused under certain configurations.

%\subsubsection{The Performance of Configurations} 

\begin{comment}
\subsection{Configuration-related performance bug}
Another line of research considers the debugging of performance errors that are caused by configuration options.
Attariyan et al.~\cite{RN3253} proposed an approach based on dynamic taint analysis technique to identify the option that causes a performance error. % assigns to each source code block a cost, use a dynamic analysis techniques that instruments and runs a software system, then their approach identifies which blocks were executed and which options they depends on. 
\end{comment}

\begin{comment}
Another line of research considers the identification of the optimal configurations for a software system and the debugging of performance errors that are caused by configuration options. 


Attariyan et al.~\cite{RN3253} proposed an approach based on dynamic taint analysis technique to identify the option that causes a performance error. % assigns to each source code block a cost, use a dynamic analysis techniques that instruments and runs a software system, then their approach identifies which blocks were executed and which options they depends on. 
Siegmund et al.~\cite{RN2880} build mathematical models that describe the impact of a configuration on software performance based on each option's value. Raghavachari et al.~\cite{RN3537} propose an iterative approach to identify an optimal configuration in terms of performance. Their approach consists of selecting for a J2EE web application a first configuration, compare its performance to a second configuration until the optimal configuration is found. Similarly, Dia et al.~\cite{RN3543} propose an approach that automatically adjusts the values of existing configuration options at run-time to optimize the CPU and memory usage objectives.

Li et al.~\cite{LiAutoConfig} leveraged performance monitoring data and execution logs to dynamically optimize the values of performance-related configuration options according to varying workloads in the field. Guo et al.~\cite{RN3544} leverage non-linear regression to suggest an optimal configuration. However, collecting a large amount of data for training a model that predicts the performance of a configuration is expensive. Therefore, Sarkar et al.~\cite{RN3089} evaluated the progressive and projective sampling to train a model that predicts the performance of configuration. For their initial training sample, they consider data on which each option is enabled at least once. Other efforts identified the optimal configuration options in terms of performance by leveraging existing optimization approaches, i.e., iterative search~\cite{RN3545}, multi-objective optimization~\cite{singh2016optimizing}, and smart hill climbing~\cite{RN3518}.
\end{comment}



\begin{comment}

\subsection{Software Performance}

Performance is an important aspect of software quality. Extensive prior research has been conducted to study software performance. In this subsection, we summarize the empirical studies on  %performance to 
understanding software performance and the studies on %. We then present the related works of 
performance regression detection.

\subsubsection{Empirical Studies on Software Performance}
Several empirical studies have been conducted on the performance of software systems~\cite{ICSE2014:Huang,Jin:2012,MSR11:Zaman,MSR12:Zaman,DBLP:conf/kbse/HanYL18,Leitner2017ICPE}. For instance, Jin \emph{et al$.$}~\cite{Jin:2012} studied 109 real-world performance issues that are reported from five open source software systems and %. Based on the studied 109 performance bugs, the authors 
%developed an automated tool 
proposed an approach to detect performance issues. Zaman \emph{et al$.$}~\cite{MSR11:Zaman,MSR12:Zaman} conducted qualitative and quantitative studies on performance issues. They found that developers and users face problems in reproducing performance bugs %. More time is 
as they spend % 
a lot of time discussing performance bugs %than 
compared to other kinds of bugs (e.g., functional bugs). %\jinfu{For example, Firefox performance bugs take more time to discuss and fix.\med{I meant examples of other kinds of bugs}} 
Huang \emph{et al$.$}~\cite{ICSE2014:Huang} %studied real world performance issues. They 
proposed an approach to improve the efficiency of performance regression testing by leveraging a static analysis technique to estimate the risk of a given commit in introducing a performance regression. Han et al$.$~\cite{DBLP:conf/kbse/HanYL18} studied %y
300 bug reports from three large open source projects. The authors found that most of the performance bugs occur for specific combinations of data input and configurations. They also proposed a framework named \emph{PerfLearning} to extract such data input and configurations from bug reports to generate test frames. Leitner et al$.$~\cite{Leitner2017ICPE} %aim to understand the current state of art of performance testing. They conduct a study on 111 open-source java-based systems from GitHub 
investigated the state-of-the-practice related to performance tests. The authors found that performance tests form only a small portion of a test suite.
%and use a combination of quantitative and qualitative research methods to investigate the use of performance tests across five perspectives.

The vast amount of research on software performance signifies its importance and motivates our work. %Prior studies on performance typically are based on either limited performance issue reports or release of the software. 
Different from prior research, we evaluate software performance at the commit level and study performance regressions that are manifested under a subset of the possible configurations. % by configuration option.
In addition, our work is different from this line of research as we consider how to avoid performance regressions that are related to some configurations while being hidden by other configurations, instead of understanding the existing performance related issues. 

\subsubsection{Performance regression detection}
Extensive prior research has proposed automated techniques to detect performance regressions. Such detection techniques can be divided into two categories: measurement-based and model-based detection. 

Measurement-based approaches compare performance metrics (e.g., CPU usage) between two consecutive versions to detect performance regressions. %measure performance metrics and compares these performance metrics between two consecutive versions of a system to detect performance regression. 
For example, Nguyen \emph{et al$.$}~\cite{Nguyen:2012:ADP,nguyen2011automated,Nguyen:2014:ICS} %conducted a series of studies on performance regressions. Nguyen \emph{et al$.$} apply control charts to analyze performance counters across test runs to detect performance regression automatically. They construct the control chart to detect performance regressions  by setting upper and lower bounds of performance counters.
leveraged control charts to identify performance regressions. %~\heng{treat regression as a countable word throughout the paper, countable seems better}. 
A control chart has an upper control limit and a lower control limit. A performance regression is detected when a performance metric is above the upper limit or below the lower limit. Foo \emph{et al$.$}~\cite{foo2010mining} proposed an approach that compares a test's performance metrics to historical performance metrics. %\med{from what?}performance regression testing repositories to detect potential performance regressions. 

A model-based approach builds %\med{This paragraph is difficult to understand. What is detected model? is it prediction model? and what is counters? what is signatures? what do you mean by heterogeneous environment?} 
%\jinfu{detected model is a general model, we can use model. Performance counter is performance metric, like CPU usage. Some papers use signatures to represent system users' behavior.} \med{I updated this paragraph, though Bodik work is not clear} 
a machine learning model with a set of performance metrics to detect performance regressions.  Cohen et al.~\cite{Cohen:2005:CIC} showed an implication that it is ineffective and not enough to index and identify performance problems with simple records of raw system metrics. Cohen et al. used TAN (Tree-Augmented Bayesian Network) models to model the system performance states based on a small subset of metrics. %Therefore, the authors present an approach to capture signatures representing system states from a running system and cluster such signitures to detect recurrent or similar performance problems. 
Bodik \emph{et al$.$}~\cite{bodik2008hilighter} leveraged a logistic regression model to model system users' behavior to improve Cohen \emph{et al$.$}\textquotesingle s model. %~\heng{need to mention Cohen's model beforehand}. 
Foo \emph{et al$.$}~\cite{DBLP:conf/icse/FooJAHZF15} proposed an approach that uses ensembles of models to detect performance regressions in heterogeneous environments (e.g., different hardware and software configurations). % \med{examples of what heterogeneous environments} \jinfu{different hardware and software configurations}). 
Xiong \emph{et al$.$}~\cite{Xiong:2013:VAM} proposed a model-driven framework to diagnose application performance and identify the root cause of performance issues. %Such framework uses linear regression to build the predict model to automatically diagnose the system performance in cloud environment and lead to root cause of performance problem.

%\med{well, we predict it, so it's kind of detecting performance regression}Our research is not designed to detect performance regression. The goal of our research is to examine the impact of configuration options on the performance regression. In our paper, we conduct measurement-based approach to identify performance regression on the commit level.

%\med{is the following paragraph correct?}
Our work complements this line of research in the sense that we consider the configuration aspect of highly configurable software systems. For instance, a code change might not show a performance regression on the default configuration, while leading to regressions on other configurations. This paper sheds light on the \inconsistent problem by first quantifying the existence of inconsistent performance variations, then proposing a prediction model that identifies the commits, tests, and options that exhibit the \inconsistent problem. 

\end{comment}