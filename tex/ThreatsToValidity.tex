
\section{Threats to Validity}
\label{sec:threats}
In this section, we discuss the threats to the validity of our study. 

\noindent \textbf{External validity.} The first external threat to validity concerns the generalizability of our results to other software systems. Due to the expensive computing resources needed (we spent around 12,536 machine hours collecting performance data), we conducted our evaluation on two open-source software systems, i.e., \emph{Hadoop} and \emph{Cassandra}. Our findings may not generalize to other software systems. %Our evaluated software systems are implemented in \emph{Java}. Therefore, our findings might not be generalizable to software systems implemented in other programming languages.
However, we found motivating results on the prevalence of \inconsistent and the performance of our prediction model, which can be replicated by future studies on other software systems. % can evaluate our approach and findings on commercial software systems with other languages. 

\noindent \textbf{Internal validity.}
When we collect our commits data, We only consider the last commit, if multiple commits are associated with the same issue. We may miss some commits with code changes. Future work can study the difference if considering all the commits.
Another internal threat to validity concerns the performance metrics that we consider. In our approach, we collect five popular performance metrics, i.e., Response time, CPU, Memory, I/O read and write, while other performance metrics such as throughput can still be explored by future research.% can choose other performance metrics to complement our study. 
We do not consider the combination of configuration options as that will require a huge cost and because the goal of our study is to identify and define the \inconsistent issues. On the one hand, a prior work~\cite{RN2864} mentions that 72\% of the performance issues are due to a single option, so our paper is covering the most common cause of performance issues. On the other hand, one can use covering arrays to conduct N-way testing for functional tests but with very low number of cases~\cite{colbourn2004combinatorial}. However, for performance testing, the approach likely does not work since we want to isolate each combination's performance impact from others. We encourage future studies to extend our work by considering the interaction of configuration options.

Similarly, our prediction model does not cover all the possible dimensions of metrics. For example, we do not consider a developer dimension. However, our model shows a good AUC performance to predict whether a \instance manifests an \inconsistent. Future studies can explore more dimensions of metrics to improve the performance of our models. 

Finally, our evaluation considers just the traditional models (i.e., logistic regression, random forest, and XGBoost) and neural network models (i.e., general neural network and convolutional neural network). Although we do not cover all existing models, our study covers the most popular ones that are used in software engineering. Future work is encouraged to explore more models. 


%In RQ2, we use four dimension metrics, i.e., code change, code structure, code token, and configuration option to build our prediction model. However, there exist other dimension metrics such as metrics from developer.  

%We choose three traditional models, i.e., logistic regression, random forest, and XGBoost and two neural network models, i.e., general neural network and convolutional neural network to construct our predictors. There are more machine learning algorithms can be used to build our model. Future research can add more metrics and use other machine learning algorithms to improve the results.

\noindent \textbf{Construct validity.} %Our evaluation is based on the stability and correctness of collected performance data. The quality and correctness of recorded performance data can impact our study. We assume that the performance monitoring tool named \emph{psutil} can accurately capture performance metrics. \emph{psutil} has been widely used in software performance study~\cite{DBLP:conf/icsm/AlghmadiSSH16, DBLP:conf/icsm/ChenS17}. 
% There might exist environmental performance noise when we use \emph{psutil} to capture the performance data. To minimize the noise, we capture the performance of the corresponding Linux process of the running tests. Furthermore, for each test, we repeat the execution 30 times independently. %\med{is this correct?}
% Finally, we run all of our experiments in the same environments. %\bram{what does ``configuration'' mean here?}. 
% \jinfu{ }
The stability of the cloud-based testing environment may cause testing noise and pose a threat to the validity of the measured performance data. To minimize the noise, we capture the performance of the corresponding Linux process of the running tests. Furthermore, for each test, we repeat the execution 30 times independently. Finally, we run all of our experiments in the same environments. There may still exist extreme values as outliers that should not be considered by our approach. To mitigate this threat, we remove the outlier data using the mean ± 3 × standard deviation (STD) as an indicator of outliers.
