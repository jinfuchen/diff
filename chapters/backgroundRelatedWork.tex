
\section{Background}
\label{sec:back}


%\subsection{Configuration}

Software configuration is a mechanism used to customize the behaviour of a software system without changing the source code. The configuration \textbf{options} are often stored in configuration files as a set of key - value pairs, where the key represents an option's name and the \textbf{value} represents a default or user-chosen value for that option. We define a \textbf{configuration} as one user choice to all the existing options. Table~\ref{tab:terms} lists the definition of these terms. For example, \emph{A=1} and \emph{B=2} is one possible configuration for a software system with the two integer options \emph{A} and \emph{B}. For instance, users can adapt the execution of their software systems by simply modifying the values of certain configuration options. For example, a user can change the directory that stores the cache for \emph{Cassandra} by changing the value of the \textit{saved\_caches\_directory} configuration option. Such configuration can be changed at run-time without changing or recompiling the source code of the whole software system.

\begin{table}[ht]
    \centering
    \caption{Our definition of configuration, option, and value}
    \begin{tabular}{l|p{6.6cm}|l}
        \hline
        Term & Definition & Example \\
        \hline
        \textbf{Option}  & A configurable item that allows users to set different values. & $A$ \\
        \textbf{Value} & A specific choice of value for an option. & $A = 1$ \\
        \textbf{Configuration} & One user choice for all the configurable options. & $A = 1; B = 2$ \\
        \hline
    \end{tabular}
    \label{tab:terms}
\end{table}


Although configuration introduces a large flexibility for users, considering all the possible configurations during testing is impossible. A software system with only 3 boolean configuration options requires testing 2${^3}$ configurations. In fact, configuration problems are among the dominant problems in software engineering~\cite{tse,RN2897}.

In particular, a software system can suffer from what we refer to as the \textbf{Inconsistent Options Performance Variation} (a.k.a, \inconsistent). The \inconsistent occurs when for a given commit \emph{C} the performance of a subset of an option's values are different from the same values in the prior commit to \emph{C}. Considering the example in Figure~\ref{fig:description}, when comparing the raw performance of the two option values \emph{V1} and \emph{V2} (Figure~\ref{fig:description-a}), we observe that \emph{V1} show a better performance than \emph{V2}. However, that might not be problematic as \emph{V2} might just enable an extra feature, such as logging a transaction. In fact, the performance of \emph{V2} is improved compared to the prior commit, while that improvement is not manifested under the option's value \emph{V1}, as shown in Figure~\ref{fig:description-b}. %That indicates the first type of \inconsistent, which consists of an inconsistent improvement across different values of the same option. 
Contrariwise, Figure~\ref{fig:description-d}, shows that even if \emph{V2} does not show any significant performance variation from the prior commit, \emph{V1} suffers from a performance regression. 
A performance variation is calculated as the difference between the performance variation of each option's value after and before each commit, which is illustrated in Figure~\ref{fig:description} by ``$a - b$'' .

\section{Related Work}
\label{sec:relatedwork}

In this section, we discuss prior work along two dimensions: software configuration and software performance. % engineering.

\subsection{Software Configuration}

A large body of research efforts have been conducted on software configuration, which mainly focus on understanding configuration problems, preventing configuration errors, and debugging configuration errors. Few research efforts consider the performance aspect of software configuration. 

\subsubsection{Understanding Configuration Problems}
Configuration makes a software system complex~\cite{tse}, which leads to configuration errors that are severe, common, and hard to debug~\cite{RN3251}. For instance, Jin et al.~\cite{RN2897} found that configuration options add more complexity to the development and test of highly configurable software systems. Han et al.~\cite{RN2864} found that configuration options are responsible for 59\% of the performance bugs. Gousios et al.~\cite{RN3551} observed that the configuration of the garbage collectors have an impact on the performance of server applications. Furthermore, Sayagh et al.~\cite{RN3249, RN2758} found that the impact of a configuration option can spread to multiple layers of the LAMP stack.% Jin et al.~\cite{RN2897} found that there is a need for tools that debug configuration errors in multi-languages software systems. 

%Our work is different from this line of research as we consider the performance regression that is caused by configuration options.
%\subsubsection{Debugging Configuration Errors} 
A second line of research proposed and evaluated different approaches to identify misconfigured configuration options. Dong et al.\cite{RN2805, RN3163} leverage the slicing technique to identify the misconfigured option for a given error message or exception. Rabkin et al.~\cite{RN2822} leverage a data flow analysis technique to identify for each option, which source code lines it might impacts. Attaryian et al.~\cite{RN3248} combined dynamic control and data flow analysis to identify misconfigured options. Zhang et al.~\cite{RN2839, RN2777} compared the trace of a correct execution against the trace of an incorrect execution to identify culprit options. We refer to our prior systematic literature review~\cite{tse} and the work of Tianyin et al.~\cite{RN3252} and Andrzejak et al.\cite{andrzejaksoftware} for further details about the existing configuration debugging approaches. 

Our work is different from this line of research since we do not consider debugging configuration errors, but understanding and identifying performance regressions that are caused under certain configurations.

\subsubsection{The Performance of Configuration} 

Another line of research considers the identification of the optimal configurations for a software system and the debugging of performance errors that are caused by configuration options. Attariyan et al.~\cite{RN3253} proposed an approach based on dynamic taint analysis technique to identify the option that causes a performance error. % assigns to each source code block a cost, use a dynamic analysis techniques that instruments and runs a software system, then their approach identifies which blocks were executed and which options they depends on. 
Siegmund et al.~\cite{RN2880} build mathematical models that describe the impact of a configuration on software performance based on each option's value. Raghavachari et al.~\cite{RN3537} proposed an iterative approach to identify an optimal configuration in terms of performance. Their approach consists of selecting for a J2EE web application a first configuration, compare its performance to a second configuration until the optimal configuration. Similarly, Dia et al.~\cite{RN3543} proposed an approach that automatically adjusts the values of existing configuration options at run-time to optimize the CPU and memory usage objectives. Li et al.~\cite{LiAutoConfig} leveraged performance monitoring data and execution logs to dynamically optimize the values of performance-related configuration options according to varying workloads in the field. Guo et al.~\cite{RN3544} leverage non-linear regression to suggest an optimal configuration. However, collecting a large amount of data for training a model that predicts the performance of a configuration is expensive. Therefore, Sarkar et al.~\cite{RN3089} evaluated the progressive and projective sampling to train a model that predicts the performance of configuration. For their initial training sample, they consider data on which each option is enabled at least once. Other efforts identified the optimal configuration options in terms of performance by leveraging existing optimization approaches, i.e., iterative search~\cite{RN3545}, multi-objective optimization~\cite{singh2016optimizing}, and smart hill climbing~\cite{RN3518}.


The goal of our paper is not to identify optimal configuration options or debug an existing performance-related configuration error, but we focus on studying the inconsistent options performance through different commits. In particular, we focus on understanding whether a performance improvement or regression is consistent through all the values of an option. That is important, as one can improve the performance of his software system or release new changes that do not impact the performance under one configuration when other configurations hide a performance regression. 

Furthermore, prior work on this line of research compare the absolute performance between two values for the same option, while this can be subjective as discussed earlier. One option's value can naturally consumes performance as it enables the execution of some extra features. However, the execution of the software system under the same option's value can be improved compared the same option and value prior to that commit. In addition, a better performing option's value can show a regression compared to the prior commit as well. %impact of configuration options on the performance regression.% While a new commit does not show any performance regression under the default configuration, other configurations might show a low performance when the same configuration showed a good performance in the prior commit. 

\subsection{Software Performance}

Performance is an important aspects of software quality. Extensive prior research has been conducted to study software performance. In this subsection, we summarize the empirical studies on  %performance to 
understanding software performance and the studies on %. We then present the related works of 
performance regression detection.

\subsubsection{Empirical Studies on Software Performance}
Several empirical studies have been conducted on the performance of software systems~\cite{ICSE2014:Huang,Jin:2012,MSR11:Zaman,MSR12:Zaman,DBLP:conf/kbse/HanYL18,Leitner2017ICPE}. For instance, Jin \emph{et al$.$}~\cite{Jin:2012} studied 109 real world performance issues that are reported from five open source software systems and %. Based on the studied 109 performance bugs, the authors 
%developed an automated tool 
proposed an approach to detect performance issues. Zaman \emph{et al$.$}~\cite{MSR11:Zaman,MSR12:Zaman} conducted qualitative and quantitative studies on performance issues. They found that developers and users face problems in reproducing performance bugs %. More time is 
as they spend % 
a lot of time on discussing performance bugs %than 
compared to other kinds of bugs (e.g., functional bug). %\jinfu{For example, Firefox performance bugs take more time to discuss and fix.\med{I meant examples of other kinds of bugs}} 
Huang \emph{et al$.$}~\cite{ICSE2014:Huang} %studied real world performance issues. They 
proposed an approach to improve the efficiency of performance regression testing by leveraging a static analysis technique to estimate the risk of a given commit in introducing a performance regression. Han et al$.$~\cite{DBLP:conf/kbse/HanYL18} studied %y
300 bug reports from three large open source projects. The authors found that most of the performance bugs occur for a specific combinations of data input and configurations. They also proposed a framework named \emph{PerfLearning} to extract such data input and configurations from bug reports to generate test frames. Leitner et al$.$~\cite{Leitner2017ICPE} %aim to understand the current state of art of performance testing. They conduct a study on 111 open-source java-based systems from GitHub 
investigated the state-of-the-practices that are related to performance tests. The authors found that performance tests form only a small portion of the test suite.
%and use a combination of quantitative and qualitative research methods to investigate the use of performance tests across five perspectives.

The vast amount of research on software performance signify its importance and motivate our work. %Prior studies on performance typically are based on either limited performance issue reports or release of the software. 
Different from prior research, we evaluate software performance at the commit level and study performance regressions that are manifested under a subset of the possible configurations. % by configuration option.
In addition, our work is different from this line of research as we consider how to avoid performance regressions that are related to some configurations while being hidden by other configurations, instead of understanding the existing performance related issues. 

\subsubsection{Performance regression detection}
Extensive prior research has proposed automated techniques to detect performance regressions. Such detection techniques can be divided into two categories: measurement-based and model-based detection. 

Measurement-based approaches compare performance metrics (e.g., CPU usage) between two consecutive versions to detect performance regressions. %measure performance metrics and compares these performance metrics between two consecutive versions of a system to detect performance regression. 
For example, Nguyen \emph{et al$.$}~\cite{Nguyen:2012:ADP,nguyen2011automated,Nguyen:2014:ICS} %conducted a series of studies on performance regressions. Nguyen \emph{et al$.$} apply control charts to analyze performance counters across test runs to detect performance regression automatically. They construct the control chart to detect performance regressions  by setting upper and lower bounds of performance counters.
leveraged control charts to identify performance regressions. %~\heng{treat regression as a countable word throughout the paper, countable seems better}. 
A control chart has a upper control limit and a lower control limit. A performance regression is detected when a performance metric is above the upper limit or bellow the lower limit. Foo \emph{et al$.$}~\cite{foo2010mining} proposed an approach that compares a test's performance metrics to historical performance metrics. %\med{from what?}performance regression testing repositories to detect potential performance regressions. 

Model-based approach builds %\med{This paragraph is difficult to understand. What is detected model? is it prediction model? and what is counters? what is signatures? what do you mean by heterogeneous environment?} 
%\jinfu{detected model is a general model, we can use model. Performance counter is performance metric, like CPU usage. Some papers use signatures to represent system users' behavior.} \med{I updated this paragraph, though Bodik work is not clear} 
a machine learning model with a set of performance metrics to detect performance regressions.  
Cohen et al.~\cite{Cohen:2005:CIC} showed an implication that it is ineffective and not enough to index and identify performance problems with simple records of raw system metrics. Cohen et al. used TAN (Tree-Augmented Bayesian Network) models to model the system performance states based a small subset of metrics. %Therefore, the authors present an approach to capture signatures representing system states from a running system and cluster such signitures to detect recurrent or similar performance problems. 
Bodik \emph{et al$.$}~\cite{bodik2008hilighter} leveraged logistic regression model to model system users' behavior to improve Cohen \emph{et al$.$}\textquotesingle s model. %~\heng{need to mention Cohen's model beforehand}. 
Foo \emph{et al$.$}~\cite{DBLP:conf/icse/FooJAHZF15}  proposed an approach that uses ensembles of models to detect performance regressions in heterogeneous environments (e.g., different hardware and software configurations). % \med{examples of what heterogeneous environments} \jinfu{different hardware and software configurations}). 
Xiong \emph{et al$.$}~\cite{Xiong:2013:VAM} proposed a model-driven framework to diagnose application performance and identify the root cause of performance issues. %Such framework uses linear regression to build the predict model to automatically diagnose the system performance in cloud environment and lead to root cause of performance problem.

%\med{well, we predict it, so it's kind of detecting performance regression}Our research is not designed to detect performance regression. The goal of our research is to examine the impact of configuration options on the performance regression. In our paper, we conduct measurement-based approach to identify performance regression on the commit level.

%\med{is the following paragraph correct?}
Our work complements this line of research in the sense that we consider the configuration aspect of highly configurable software systems. For instance, a code change might not show a performance regression on the default configuration, while leading to regressions on other configurations. This paper sheds light on the \inconsistent problem by first quantifying the existence of inconsistent performance variations, then proposing a prediction model that identifies the commits, tests, and options that exhibit the \inconsistent problem. 
