
\section{Threats to Validity}
\label{sec:threats}
In this section, we discuss the threats to the validity of our study. 

\noindent \textbf{External validity.} The first external threat to validity concerns the generalizability of our results to other software systems. Due to the expensive computing resources needed (we spent around 12,536 machine hours collecting performance data), we conducted our evaluation on two open-source software systems, i.e., \emph{Hadoop} and \emph{Cassandra}. Our findings may not generalize to other software systems. %Our evaluated software systems are implemented in \emph{Java}. Therefore, our findings might not be generalizable to software systems implemented in other programming languages.
However, we found motivating results on the prevalence of \inconsistent and the performance of our explanatory model, which can be replicated by future studies on other software systems. % can evaluate our approach and findings on commercial software systems with other languages. 

\noindent \textbf{Internal validity.}
When we collect our commits data, We only consider the last commit, if multiple commits are associated with the same issue. We may miss some commit with code change. Future work can study the difference if considering all the commits.

Another internal threat to validity concerns the performance metrics that we consider. In our approach, we collect five popular performance metrics, i.e., Response time, CPU, Memory, I/O read and write, while other performance metrics such as throughput can still be explored by future research.% can choose other performance metrics to complement our study. 


Similarly, our explanatory model does not cover all the possible dimensions of metrics. For example, we do not consider a developer dimension. However, our model shows a good AUC performance to predict whether a \instance manifests an \inconsistent. Future studies can explore more dimensions of metrics to improve the performance of our models. 

Finally, our evaluation considers just the traditional models (i.e., logistic regression, random forest, and XGBoost) and neural network models (i.e., general neural network and convolutional neural network). Although we do not cover all existing models, our study covers the most popular ones that are used in software engineering. Future work is encouraged to explore more models. 


%In RQ2, we use four dimension metrics, i.e., code change, code structure, code token, and configuration option to build our prediction model. However, there exist other dimension metrics such as metrics from developer.  

%We choose three traditional models, i.e., logistic regression, random forest, and XGBoost and two neural network models, i.e., general neural network and convolutional neural network to construct our predictors. There are more machine learning algorithms can be used to build our model. Future research can add more metrics and use other machine learning algorithms to improve the results.

\noindent \textbf{Construct validity.} %Our evaluation is based on the stability and correctness of collected performance data. The quality and correctness of recorded performance data can impact our study. We assume that the performance monitoring tool named \emph{psutil} can accurately capture performance metrics. \emph{psutil} has been widely used in software performance study~\cite{DBLP:conf/icsm/AlghmadiSSH16, DBLP:conf/icsm/ChenS17}. 
There might exist environmental performance noise when we use \emph{psutil} to capture the performance data. To minimize the noise, we capture the performance of the corresponding Linux process of the running tests. Furthermore, for each test, we repeat the execution 30 times independently. %\med{is this correct?}
Finally, we run all of our experiments in the same environments. %\bram{what does ``configuration'' mean here?}.
